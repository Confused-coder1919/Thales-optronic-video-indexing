# CONTEXT PACK

Generated by `scripts/make_context_pack.py`.

## Repo Metadata

Not a git repository or git not available.

## Python & Environment

- Python: `3.11.14`
- pip freeze (first 80 lines):

```
accelerate==1.12.0
altair==6.0.0
annotated-types==0.7.0
anyio==4.12.0
attrs==25.4.0
blinker==1.9.0
cachetools==6.2.4
certifi==2026.1.4
charset-normalizer==3.4.4
click==8.3.1
eval_type_backport==0.3.1
ffmpeg-python==0.2.0
filelock==3.20.2
fsspec==2025.12.0
future==1.0.0
gitdb==4.0.12
GitPython==3.1.46
googleapis-common-protos==1.72.0
h11==0.16.0
hf-xet==1.2.0
httpcore==1.0.9
httpx==0.28.1
huggingface-hub==0.36.0
idna==3.11
importlib_metadata==8.7.1
invoke==2.2.1
Jinja2==3.1.6
jsonschema==4.25.1
jsonschema-specifications==2025.9.1
MarkupSafe==3.0.3
mistralai==1.10.0
mpmath==1.3.0
narwhals==2.14.0
networkx==3.6.1
numpy==2.2.6
opencv-python==4.12.0.88
opentelemetry-api==1.38.0
opentelemetry-exporter-otlp-proto-common==1.38.0
opentelemetry-exporter-otlp-proto-http==1.38.0
opentelemetry-proto==1.38.0
opentelemetry-sdk==1.38.0
opentelemetry-semantic-conventions==0.59b0
packaging==25.0
pandas==2.3.3
pillow==12.1.0
protobuf==6.33.2
psutil==7.2.1
pyarrow==22.0.0
pydantic==2.12.5
pydantic_core==2.41.5
pydeck==0.9.1
python-dateutil==2.9.0.post0
python-dotenv==1.2.1
pytz==2025.2
PyYAML==6.0.3
referencing==0.37.0
regex==2025.11.3
requests==2.32.5
rpds-py==0.30.0
safetensors==0.7.0
six==1.17.0
smmap==5.0.2
streamlit==1.52.2
sympy==1.14.0
tenacity==9.1.2
# Editable install with no version control (thales==1.0.0)
-e /Users/syedtashfin/Documents/GitHub/thales-optronic-video-indexing-main
tokenizers==0.22.2
toml==0.10.2
torch==2.9.1
torchvision==0.24.1
tornado==6.5.4
tqdm==4.67.1
transformers==4.57.3
typing-inspection==0.4.2
typing_extensions==4.15.0
tzdata==2025.3
urllib3==2.6.2
zipp==3.23.0
```

## Directory Tree (Depth 4)

```
thales-optronic-video-indexing-main/
  notebooks/
    01_speech_to_text.ipynb
    02_video_pipeline.ipynb
  reports_demo/
    summary_report.json
    video_1_report.json
  reports_demo2/
    summary_report.json
    video_1_report.json
  reports_real/
    summary_report.json
    video_1_report.json
  scripts/
    convert_transcript_to_utf8.py
    make_context_pack.py
  thales/
    __init__.py
    __main__.py
    cli.py
    config.py
    entity_categorizer.py
    entity_detector.py
    entity_extractor.py
    report_generator.py
    utils_io.py
    video_processor.py
    voice_parser.py
  thales.egg-info/
    dependency_links.txt
    entry_points.txt
    PKG-INFO
    requires.txt
    SOURCES.txt
    top_level.txt
  ui/
    work/
      run_1767642611_pair_1/
        voice_1.txt
      run_1767643307_pair_1/
        voice_1.txt
    app.py
    requirements-ui.txt
    streamlit.log
    utils.py
  .DS_Store
  .gitignore
  CONTEXT_PACK.md
  main.py
  pyproject.toml
  README.md
  requirements.txt
```

## Entry Points & How to Run (README excerpt)

````markdown
# üé• Thales Video Indexing

## üìò Description

Ce projet a pour objectif de cr√©er une **base de vid√©os annot√©e automatiquement** √† partir d‚Äôun dataset interne.
Deux pipelines sont utilis√©s en parall√®le :

- **Speech-to-Text (STT)** pour la transcription et la traduction de l‚Äôaudio.
- **Image-to-Text (Vision)** pour la description des frames et la d√©tection d‚Äôobjets.

Le r√©sultat final est un fichier unique **`metadata_final.csv`** contenant les m√©tadonn√©es fusionn√©es (audio + vid√©o).

---

## üß± Structure du projet

```bash
thales-video-indexing/
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îú‚îÄ .gitignore
‚îÇ
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ videos/                       # .mp4 sources (non versionn√©s)
‚îÇ  ‚îú‚îÄ audio/                        # .wav extraits automatiquement
‚îÇ  ‚îú‚îÄ frames/                       # frames extraites automatiquement
‚îÇ  ‚îî‚îÄ metadata/                     # m√©tadonn√©es & sorties finales
‚îÇ      ‚îú‚îÄ manifest.csv              # inventaire auto des vid√©os
‚îÇ      ‚îú‚îÄ stt_<video_id>.csv        # r√©sultats speech-to-text
‚îÇ      ‚îú‚îÄ vision_<video_id>.csv     # r√©sultats image-to-text / d√©tection
‚îÇ      ‚îî‚îÄ metadata_final.csv        # fichier fusionn√© final
‚îÇ
‚îú‚îÄ notebooks/
‚îÇ  ‚îú‚îÄ 01_speech_to_text.ipynb       # pipeline audio (Whisper/Faster-Whisper)
‚îÇ  ‚îî‚îÄ 02_video_pipeline.ipynb       # pipeline vid√©o (VLM/objets)
‚îÇ
‚îî‚îÄ src/
   ‚îú‚îÄ __init__.py                   # vide (n√©cessaire pour les imports)
   ‚îú‚îÄ dataset_preparation.py        # pr√©paration dataset (scan, manifest, frames, audio)
   ‚îî‚îÄ fusion.py                     # fusion STT + Vision ‚Üí metadata_final.csv
```
````

---

## ‚öôÔ∏è Installation

```bash
# Cloner le d√©p√¥t
git clone https://github.com/<ton-utilisateur>/thales-video-indexing.git
cd thales-video-indexing

# Cr√©er un environnement virtuel (optionnel)
python -m venv venv
source venv/bin/activate   # (Linux/macOS)
venv\Scripts\activate      # (Windows)

# Installer les d√©pendances
pip install -r requirements.txt
```

---

## üöÄ Utilisation

### 1Ô∏è‚É£ Pr√©paration du dataset

Ce script scanne le dossier `data/videos/`, cr√©e le `manifest.csv`, extrait des frames et des pistes audio.

```bash
python -m src.dataset_preparation --scan --manifest --extract-frames --extract-audio
```

### 2Ô∏è‚É£ Lancer les pipelines (dans Jupyter)

- **01_speech_to_text.ipynb** ‚Üí g√©n√®re `stt_<video_id>.csv`
- **02_video_pipeline.ipynb** ‚Üí g√©n√®re `vision_<video_id>.csv`

### 3Ô∏è‚É£ Fusion des r√©sultats

Fusionne les r√©sultats audio et vid√©o dans un seul CSV final :

```bash
python -m src.fusion
```

üü¢ Sortie :
`data/metadata/metadata_final.csv`

---

## üìä R√©sultat attendu

Le fichier `metadata_final.csv` regroupe toutes les m√©tadonn√©es audio et vid√©o sous un format standardis√©, par exemple :

| video_id | timestamp_frame | audio_transcription  | video_description                   | video_objects | confidence |
| -------- | --------------- | -------------------- | ----------------------------------- | ------------- | ---------- |
| 001      | 12.0            | "A drone is flying." | "A small drone appears in the sky." | drone         | 0.87       |

---

## üß© Auteurs

- **Chlo√© de Wilde** ‚Äî Data & AI Engineering
- Projet acad√©mique Thales ‚Äì _Video Indexing Pipeline_

---

## üõ°Ô∏è Licence

Ce projet est r√©serv√© √† un usage acad√©mique et interne.

---

## UI (Streamlit)

Run the pipeline with a simple web UI.

```bash
python -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt
pip install -r ui/requirements-ui.txt

export MISTRAL_API_KEY=Hv4LDgQWAMkUvvhDtnO2qF7OON1oacu1  # or set it in .env
streamlit run ui/app.py
```

Notes:

- The first real run will download the Hugging Face model and requires internet access.
- The UI has a "demo mode" toggle. Turn it off for real results (requires a Mistral API key).

---

## Transcript encoding

UTF-8 is recommended; UTF-16 and common Windows encodings are supported.

To convert a transcript to UTF-8:

```bash
python scripts/convert_transcript_to_utf8.py input.txt output.txt
```

---

## Context packer

Generate a single markdown file with project context for review:

```bash
python scripts/make_context_pack.py
```

Output: `CONTEXT_PACK.md` at the repo root.

```

## CLI Help (python -m thales --help)

```

usage: **main**.py [-h] [--directory DIRECTORY] [--output OUTPUT]
[--interval INTERVAL] [--version]

Thales - Process voice transcripts and videos to detect military entities

options:
-h, --help show this help message and exit
--directory DIRECTORY, -d DIRECTORY
Directory containing voice and video files (default:
current directory)
--output OUTPUT, -o OUTPUT
Output directory for reports (default: reports/)
--interval INTERVAL, -i INTERVAL
Interval between video frames in seconds (default: 5)
--version, -v Show version and exit

Examples:
python -m thales # Process all files in current directory
python -m thales -d ./data # Process files from data directory
python -m thales -i 10 -o ./reports # 10-second intervals, output to reports/

```

## Dependencies (requirements.txt)

```

transformers>=4.51.1
torch>=2.0.0
torchvision>=0.15.0
accelerate>=1.10.0
opencv-python>=4.8.0
ffmpeg-python>=0.2.0
pandas>=2.0.0
pillow>=11.0.0
numpy>=1.24.0
mistralai>=1.0.0
python-dotenv>=1.0.0

````

## pyproject.toml

```toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "thales"
version = "1.0.0"
description = "Military entity detection pipeline using LLMs and vision models"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "Thales POC Team"}
]
keywords = ["military", "entity-detection", "computer-vision", "llm", "video-analysis"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]

dependencies = [
    "mistralai>=1.0.0",
    "opencv-python>=4.8.0",
    "numpy>=1.24.0",
    "Pillow>=10.0.0",
    "python-dotenv>=1.0.0",
    "torch>=2.0.0",
    "transformers>=4.30.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "mypy>=1.0.0",
]

[project.scripts]
thales = "thales.cli:main"

[project.urls]
Homepage = "https://github.com/thales/entity-detection"
Documentation = "https://github.com/thales/entity-detection#readme"

[tool.setuptools.packages.find]
where = ["."]
include = ["thales*"]

[tool.black]
line-length = 100
target-version = ["py310", "py311", "py312"]

[tool.isort]
profile = "black"
line_length = 100

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
ignore_missing_imports = true


````

## Code: thales/**init**.py

```python
"""
Thales - Military Entity Detection Pipeline

A pipeline for processing voice transcripts and videos to detect
military entities using LLMs and vision models.
"""

__version__ = "1.0.0"
__author__ = "Thales POC Team"

from thales.config import (
    ENTITY_CATEGORIES,
    ENTITY_NORMALIZATION,
    VALID_CATEGORIES,
)

__all__ = [
    "__version__",
    "ENTITY_CATEGORIES",
    "ENTITY_NORMALIZATION",
    "VALID_CATEGORIES",
]


```

## Code: thales/**main**.py

```python
"""
Allow running the package as a module: python -m thales
"""

from thales.cli import main

if __name__ == "__main__":
    main()


```

## Code: thales/cli.py

```python
"""
Command-line interface for the Thales entity detection pipeline.
"""

import argparse
from pathlib import Path
from typing import List, Tuple

from thales.config import DEFAULT_FRAME_INTERVAL, DEFAULT_OUTPUT_DIR
from thales.entity_detector import process_video_with_voice
from thales.report_generator import generate_report, generate_summary_report


def find_voice_video_pairs(directory: str = ".") -> List[Tuple[str, str]]:
    """
    Find all pairs of voice files and corresponding video files.

    Args:
        directory: Directory to search in

    Returns:
        List of (voice_file_path, video_file_path) tuples
    """
    dir_path = Path(directory)
    voice_files = sorted(dir_path.glob("voice_*.txt"))

    pairs = []
    for voice_file in voice_files:
        voice_stem = voice_file.stem
        try:
            number = voice_stem.split("_")[1]
        except IndexError:
            print(f"Warning: Could not parse number from {voice_file}")
            continue

        video_patterns = [
            dir_path / f"video_{number}.mkv",
            dir_path / f"video_{number}.mp4",
            dir_path / f"video_{number}.avi",
            dir_path / f"video_{number}.mov",
        ]

        video_file = None
        for pattern in video_patterns:
            if pattern.exists():
                video_file = pattern
                break

        if video_file:
            pairs.append((str(voice_file), str(video_file)))
            print(f"Found pair: {voice_file.name} <-> {video_file.name}")
        else:
            print(f"Warning: No corresponding video found for {voice_file.name}")

    return pairs


def process_all_videos(
    directory: str = ".",
    output_dir: str = DEFAULT_OUTPUT_DIR,
    interval_seconds: int = DEFAULT_FRAME_INTERVAL
):
    """
    Process all voice/video pairs and generate reports.

    Args:
        directory: Directory containing voice and video files
        output_dir: Directory to save reports
        interval_seconds: Interval between video frames to analyze
    """
    print("=" * 60)
    print("Thales - Video Entity Detection Pipeline")
    print("=" * 60)

    print("\n1. Finding voice/video pairs...")
    pairs = find_voice_video_pairs(directory)

    if not pairs:
        print("No voice/video pairs found!")
        return

    print(f"Found {len(pairs)} pair(s) to process\n")

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    all_reports = []

    for i, (voice_file, video_file) in enumerate(pairs, 1):
        print("=" * 60)
        print(f"Processing pair {i}/{len(pairs)}: {Path(video_file).name}")
        print("=" * 60)

        try:
            detection_results = process_video_with_voice(
                video_file,
                voice_file,
                interval_seconds=interval_seconds
            )

            if not detection_results:
                print(f"Warning: No detection results for {video_file}")
                continue

            report_filename = f"{Path(video_file).stem}_report.json"
            report_path = output_path / report_filename

            report = generate_report(
                video_file,
                detection_results,
                str(report_path)
            )

            all_reports.append(report)

            # Print summary
            print(f"\nSummary for {Path(video_file).name}:")
            for entity, data in report["entities"].items():
                stats = data["statistics"]
                print(f"  {entity}:")
                print(f"    Present in {stats['frames_with_entity']}/{stats['total_frames_analyzed']} frames "
                      f"({stats['presence_percentage']:.1f}%)")
                if data["time_ranges"]:
                    print(f"    Time ranges: {len(data['time_ranges'])}")
                    for tr in data["time_ranges"][:3]:
                        print(f"      - {tr['start']} to {tr['end']} ({tr['duration_seconds']}s)")
                    if len(data["time_ranges"]) > 3:
                        print(f"      ... and {len(data['time_ranges']) - 3} more")

        except Exception as e:
            print(f"Error processing {video_file}: {e}")
            import traceback
            traceback.print_exc()
            continue

    if all_reports:
        print("\n" + "=" * 60)
        print("Generating summary report...")
        print("=" * 60)

        summary_path = output_path / "summary_report.json"
        summary = generate_summary_report(all_reports, str(summary_path))

        print(f"\nSummary:")
        print(f"  Processed {summary['total_videos']} video(s)")
        print(f"  Found {summary['unique_entity_count']} unique entity type(s)")
        print(f"  All entities: {', '.join(summary['all_entities'])}")

    print("\n" + "=" * 60)
    print("Processing complete!")
    print("=" * 60)


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Thales - Process voice transcripts and videos to detect military entities",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python -m thales                     # Process all files in current directory
  python -m thales -d ./data           # Process files from data directory
  python -m thales -i 10 -o ./reports  # 10-second intervals, output to reports/
        """
    )
    parser.add_argument(
        "--directory", "-d",
        type=str,
        default=".",
        help=f"Directory containing voice and video files (default: current directory)"
    )
    parser.add_argument(
        "--output", "-o",
        type=str,
        default=DEFAULT_OUTPUT_DIR,
        help=f"Output directory for reports (default: {DEFAULT_OUTPUT_DIR}/)"
    )
    parser.add_argument(
        "--interval", "-i",
        type=int,
        default=DEFAULT_FRAME_INTERVAL,
        help=f"Interval between video frames in seconds (default: {DEFAULT_FRAME_INTERVAL})"
    )
    parser.add_argument(
        "--version", "-v",
        action="store_true",
        help="Show version and exit"
    )

    args = parser.parse_args()

    if args.version:
        from thales import __version__
        print(f"Thales Entity Detection Pipeline v{__version__}")
        return

    process_all_videos(
        directory=args.directory,
        output_dir=args.output,
        interval_seconds=args.interval
    )


if __name__ == "__main__":
    main()


```

## Code: thales/config.py

```python
"""
Configuration and constants for the Thales entity detection pipeline.

This module centralizes all configuration, constants, and entity mappings
used throughout the pipeline.
"""

import os
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# =============================================================================
# API Configuration
# =============================================================================

MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
MISTRAL_MODEL = "mistral-large-latest"
PIXTRAL_MODEL = "pixtral-large-latest"
DEMO_MODE = os.getenv("THALES_DEMO_MODE", "").strip().lower() in {"1", "true", "yes", "on"}

# =============================================================================
# Entity Categories for Visual Detection
# =============================================================================

ENTITY_CATEGORIES = [
    "military personnel",
    "civilian",
    "military truck",
    "armored vehicle",
    "artillery vehicle",
    "military vehicle",
    "trailer",
    "aircraft",
    "helicopter",
    "drone",
    "weapon",
    "turret",
    "equipment",
]

# =============================================================================
# Entity Normalization Mappings
# =============================================================================

# Map specific terms to high-level categories
ENTITY_NORMALIZATION = {
    # Military Personnel
    "driver": "military personnel",
    "signaler": "military personnel",
    "crew": "military personnel",
    "crew member": "military personnel",
    "technician": "military personnel",
    "operator": "military personnel",
    "worker": "military personnel",
    "loader": "military personnel",
    "gunner": "military personnel",
    "commander": "military personnel",
    "officer": "military personnel",
    "soldier": "military personnel",
    "troop": "military personnel",
    "troops": "military personnel",
    "personnel": "military personnel",
    "man": "military personnel",
    "woman": "military personnel",
    "person": "military personnel",

    # Civilians
    "civilian": "civilian",
    "bystander": "civilian",
    "passerby": "civilian",
    "spectator": "civilian",

    # Vehicles
    "semi truck": "military truck",
    "transport truck": "military truck",
    "logistics truck": "military truck",
    "daf truck": "military truck",
    "daf semi truck": "military truck",
    "military transport truck": "military truck",
    "logistics vehicle": "military truck",
    "tank": "armored vehicle",
    "apc": "armored vehicle",
    "ifv": "armored vehicle",
    "self-propelled artillery": "artillery vehicle",
    "artillery": "artillery vehicle",
    "howitzer": "artillery vehicle",
    "flatbed": "trailer",
    "flatbed trailer": "trailer",
    "low loader": "trailer",
    "transport trailer": "trailer",

    # Aircraft
    "uav": "drone",
    "unmanned aerial vehicle": "drone",
    "fixed-wing": "aircraft",
    "jet": "aircraft",
    "fighter": "aircraft",
    "bomber": "aircraft",

    # Weapons
    "gun": "weapon",
    "cannon": "weapon",
    "missile": "weapon",
    "rocket": "weapon",
    "main gun": "turret",
    "gun barrel": "turret",
}

# Map high-level entities to visual detection categories
ENTITY_TO_VISUAL_CATEGORY = {
    "operator": "military personnel",
    "personnel": "military personnel",
    "soldier": "military personnel",
    "military personnel": "military personnel",
    "civilian": "civilian",
    "bystander": "civilian",
    "military truck": "truck",
    "armored vehicle": "tank",
    "artillery vehicle": "artillery vehicle",
    "military vehicle": "military vehicle",
    "trailer": "trailer",
    "helicopter": "helicopter",
    "aircraft": "aircraft",
    "drone": "drone",
    "weapon": "weapon",
    "turret": "turret",
    "equipment": "equipment",
}

# Terms to exclude from entity extraction
EXCLUDED_TERMS = {
    "sweater", "jeans", "shirt", "pants", "jacket", "clothing", "clothes",
    "tracks", "wheels", "road wheels", "hull", "armor", "rear tracks",
    "travel lock", "suspension", "engine",
}

# Valid high-level categories (for normalization validation)
VALID_CATEGORIES = {
    "military personnel", "civilian", "military truck", "armored vehicle",
    "artillery vehicle", "military vehicle", "trailer", "helicopter",
    "aircraft", "drone", "weapon", "turret", "vehicle", "equipment"
}

# =============================================================================
# Processing Defaults
# =============================================================================

DEFAULT_FRAME_INTERVAL = 5  # seconds between video frames
DEFAULT_OUTPUT_DIR = "reports"
MAX_IMAGE_SIZE = 1024  # pixels for Pixtral

# =============================================================================
# Paths
# =============================================================================

def get_project_root() -> Path:
    """Get the project root directory."""
    return Path(__file__).parent.parent


def get_data_dir() -> Path:
    """Get the data directory path."""
    return get_project_root() / "data"


def get_reports_dir() -> Path:
    """Get the reports directory path."""
    return get_project_root() / "reports"

```

## Code: thales/entity_categorizer.py

```python
"""
Entity categorization using ML-based zero-shot classification.
"""

import torch
from transformers import pipeline
from typing import Dict, List

from thales.config import ENTITY_CATEGORIES, ENTITY_TO_VISUAL_CATEGORY, DEMO_MODE


def initialize_categorizer():
    """
    Initialize a zero-shot classification pipeline for entity categorization.

    Returns:
        Initialized zero-shot classification pipeline
    """
    print("Initializing entity categorizer model...")

    # Determine device
    if torch.cuda.is_available():
        device = 0
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = "mps"
    else:
        device = -1

    categorizer = pipeline(
        "zero-shot-classification",
        model="facebook/bart-large-mnli",
        device=device
    )
    print(f"Entity categorizer model loaded successfully on {device}")

    return categorizer


def categorize_entity_with_context(
    entity: str,
    context: List[str],
    categorizer
) -> str:
    """
    Categorize an entity into a visual category using ML with context.

    Args:
        entity: Entity name to categorize
        context: List of context strings where the entity appears
        categorizer: Initialized zero-shot classification pipeline

    Returns:
        Category string for visual detection
    """
    # First, check if entity is already a known high-level category
    entity_lower = entity.lower().strip()
    if entity_lower in ENTITY_TO_VISUAL_CATEGORY:
        return ENTITY_TO_VISUAL_CATEGORY[entity_lower]

    # Check if it's already a visual category
    if entity_lower in [c.lower() for c in ENTITY_CATEGORIES]:
        return entity_lower

    # Combine context for ML classification
    if context and len(context) > 0:
        context_snippets = sorted(context, key=len, reverse=True)[:3]
        full_context = " ".join(context_snippets)
        classification_text = (
            f"In the following context: '{full_context[:400]}', "
            f"the term '{entity}' refers to what type of entity?"
        )
    else:
        classification_text = f"The term '{entity}' refers to what type of entity?"

    try:
        result = categorizer(classification_text, ENTITY_CATEGORIES, multi_label=False)

        if result and 'labels' in result and len(result['labels']) > 0:
            top_label = result['labels'][0]
            top_score = result['scores'][0] if 'scores' in result else 0.0

            if top_score > 0.2:
                return top_label
            elif context:
                return categorize_entity_with_context(entity, [], categorizer)
            return top_label
        else:
            return "military personnel"

    except Exception as e:
        print(f"Error categorizing entity '{entity}' with ML: {e}")
        return "military personnel"


def categorize_entities(
    entities: List[str],
    entity_contexts: Dict[str, List[str]],
    categorizer=None
) -> Dict[str, str]:
    """
    Categorize a list of entities using ML with context.

    Args:
        entities: List of entity names
        entity_contexts: Dictionary mapping entity names to context strings
        categorizer: Optional pre-initialized categorizer

    Returns:
        Dictionary mapping entity names to their categories
    """
    if DEMO_MODE:
        print("Demo mode enabled: skipping ML categorization.")
        entity_to_category = {}
        for entity in entities:
            key = entity.lower().strip()
            entity_to_category[entity] = ENTITY_TO_VISUAL_CATEGORY.get(key, entity)
        return entity_to_category

    if categorizer is None:
        categorizer = initialize_categorizer()

    entity_to_category = {}

    print(f"Categorizing {len(entities)} entities using ML with context...")
    for i, entity in enumerate(entities):
        if (i + 1) % 10 == 0:
            print(f"  Categorized {i+1}/{len(entities)} entities...")

        context = entity_contexts.get(entity, [])
        category = categorize_entity_with_context(entity, context, categorizer)
        entity_to_category[entity] = category

        if entity != category:
            print(f"    '{entity}' -> '{category}'")

    print("Entity categorization complete")
    return entity_to_category

```

## Code: thales/entity_detector.py

```python
"""
Entity detection in video frames using Pixtral vision model.
"""

import base64
import hashlib
import io
from PIL import Image
import numpy as np
import cv2
from typing import List, Dict, Any, Optional
from mistralai import Mistral

from thales.config import MISTRAL_API_KEY, PIXTRAL_MODEL, MAX_IMAGE_SIZE, DEMO_MODE
from thales.video_processor import (
    extract_frames_at_intervals,
    get_video_duration,
    seconds_to_timestamp,
)
from thales.entity_extractor import get_entity_list, extract_entities_with_context
from thales.entity_categorizer import categorize_entities, initialize_categorizer


def get_pixtral_client() -> Optional[Mistral]:
    """
    Get an initialized Mistral client for Pixtral vision model.

    Returns:
        Initialized Mistral client, or None in demo mode

    Raises:
        ValueError: If MISTRAL_API_KEY is not configured
    """
    if DEMO_MODE:
        return None
    if not MISTRAL_API_KEY:
        raise ValueError(
            "MISTRAL_API_KEY not found in .env file. "
            "Please add MISTRAL_API_KEY=your_api_key to your .env file."
        )
    return Mistral(api_key=MISTRAL_API_KEY)


def frame_to_base64(frame: np.ndarray) -> str:
    """
    Convert OpenCV frame (BGR) to base64-encoded JPEG string.

    Args:
        frame: OpenCV frame as numpy array (BGR)

    Returns:
        Base64-encoded JPEG image string
    """
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil_image = Image.fromarray(rgb_frame)

    if pil_image.width > MAX_IMAGE_SIZE or pil_image.height > MAX_IMAGE_SIZE:
        pil_image.thumbnail((MAX_IMAGE_SIZE, MAX_IMAGE_SIZE), Image.Resampling.LANCZOS)

    buffer = io.BytesIO()
    pil_image.save(buffer, format="JPEG", quality=85)
    buffer.seek(0)

    return base64.standard_b64encode(buffer.read()).decode("utf-8")


def detect_entities_in_frame_batch(
    client: Mistral,
    frame: np.ndarray,
    entities: List[str],
    entity_to_category: Dict[str, str]
) -> Dict[str, bool]:
    """
    Detect multiple entities in a single frame using Pixtral.

    Args:
        client: Initialized Mistral client
        frame: Video frame as numpy array (BGR format)
        entities: List of entity names to detect
        entity_to_category: Dictionary mapping entity names to visual categories

    Returns:
        Dictionary mapping entity names to detection results (True/False)
    """
    try:
        image_base64 = frame_to_base64(frame)

        entity_queries = []
        for entity in entities:
            category = entity_to_category.get(entity, entity)
            entity_queries.append(f"- {entity} (look for: {category})")

        entities_list = "\n".join(entity_queries)

        prompt = f"""Analyze this image carefully and determine which of the following entities are ACTUALLY VISIBLE in the frame.

Entities to check:
{entities_list}

IMPORTANT RULES:
1. Only mark an entity as present if you can CLEARLY see it in the image
2. Be strict - if you're uncertain, mark it as NOT present
3. "military personnel" = people in military context (uniforms, operating equipment)
4. "civilian" = people in civilian clothing NOT operating military equipment
5. "military truck" = large military transport trucks (not regular cars)
6. "artillery vehicle" = self-propelled guns like AS 90, M109 (tracked vehicles with large gun)
7. "trailer" = flatbed or transport trailer carrying equipment
8. "turret" = rotating gun platform on a vehicle
9. "weapon" = visible guns, cannons, missiles

For each entity, respond with ONLY "YES" or "NO".

Format your response EXACTLY like this (one per line):
ENTITY_NAME: YES
ENTITY_NAME: NO

List ALL entities with their detection status:"""

        response = client.chat.complete(
            model=PIXTRAL_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": f"data:image/jpeg;base64,{image_base64}"
                        },
                        {
                            "type": "text",
                            "text": prompt
                        }
                    ]
                }
            ],
            temperature=0.1
        )

        content = response.choices[0].message.content.strip()
        results = {entity: False for entity in entities}

        for line in content.split('\n'):
            line = line.strip()
            if ':' in line:
                parts = line.split(':', 1)
                entity_name = parts[0].strip()
                detection = parts[1].strip().upper()

                for entity in entities:
                    if entity.lower() == entity_name.lower():
                        results[entity] = detection == "YES"
                        break

        return results

    except Exception as e:
        print(f"Error detecting entities in frame with Pixtral: {e}")
        return {entity: False for entity in entities}


def simulate_detections_for_frame(entities: List[str], second: int) -> Dict[str, bool]:
    """
    Simulate detections deterministically for demo mode.

    Args:
        entities: List of entity names to detect
        second: Second index of the frame

    Returns:
        Dictionary mapping entity names to detection results (True/False)
    """
    results: Dict[str, bool] = {}
    for entity in entities:
        digest = hashlib.md5(f"{entity}:{second}".encode("utf-8")).digest()
        results[entity] = digest[0] % 10 < 3
    return results


def detect_entities_in_video(
    video_path: str,
    entities: List[str],
    entity_to_category: Dict[str, str],
    interval_seconds: int = 1
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Detect entities in video frames at regular intervals.

    Args:
        video_path: Path to the video file
        entities: List of entity names to detect
        entity_to_category: Dictionary mapping entity names to categories
        interval_seconds: Interval between frames in seconds

    Returns:
        Dictionary mapping entity names to lists of detections
    """
    print("Initializing Pixtral vision model...")
    client = get_pixtral_client()

    if client is None:
        print("Demo mode enabled: simulating Pixtral detections.")
        duration = get_video_duration(video_path)
        total_seconds = int(duration)

        results: Dict[str, List[Dict[str, Any]]] = {entity: [] for entity in entities}
        for second in range(0, total_seconds + 1, interval_seconds):
            timestamp = seconds_to_timestamp(second)
            frame_detections = simulate_detections_for_frame(entities, second)

            for entity in entities:
                is_present = frame_detections.get(entity, False)
                results[entity].append({
                    "timestamp": timestamp,
                    "second": second,
                    "present": is_present
                })

        print("Detection complete (demo mode)")
        return results

    print(f"Extracting frames from {video_path}...")
    frames = extract_frames_at_intervals(video_path, interval_seconds)

    if not frames:
        print("No frames extracted from video")
        return {}

    print(f"Detecting {len(entities)} entities in {len(frames)} frames using Pixtral...")

    results: Dict[str, List[Dict[str, Any]]] = {entity: [] for entity in entities}

    for i, (second, frame) in enumerate(frames):
        timestamp = seconds_to_timestamp(second)
        print(f"  Processing frame {i+1}/{len(frames)} at {timestamp}...")

        frame_detections = detect_entities_in_frame_batch(
            client, frame, entities, entity_to_category
        )

        for entity in entities:
            is_present = frame_detections.get(entity, False)
            results[entity].append({
                "timestamp": timestamp,
                "second": second,
                "present": is_present
            })

    print("Detection complete")
    return results


def process_video_with_voice(
    video_path: str,
    voice_file_path: str,
    interval_seconds: int = 1
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Process a video using entities extracted from a voice file.

    Args:
        video_path: Path to the video file
        voice_file_path: Path to the corresponding voice file
        interval_seconds: Interval between frames in seconds

    Returns:
        Dictionary mapping entity names to lists of detections
    """
    print(f"Extracting entities from {voice_file_path}...")
    entities = get_entity_list(voice_file_path)

    if not entities:
        print("No entities found in voice file")
        return {}

    print(f"Found {len(entities)} entities: {', '.join(entities[:5])}{'...' if len(entities) > 5 else ''}")

    print("\nExtracting context for entities...")
    entity_contexts = extract_entities_with_context(voice_file_path)

    print("\nCategorizing entities using ML with context...")
    if DEMO_MODE:
        entity_to_category = categorize_entities(entities, entity_contexts, None)
    else:
        categorizer = initialize_categorizer()
        entity_to_category = categorize_entities(entities, entity_contexts, categorizer)

    print(f"\nDetecting entities in {video_path}...")
    results = detect_entities_in_video(
        video_path, entities, entity_to_category, interval_seconds
    )

    return results

```

## Code: thales/entity_extractor.py

```python
"""
Entity extraction from voice transcripts using Mistral LLM.
"""

import re
import json
from typing import List, Set, Dict, Optional
from mistralai import Mistral

from thales.config import (
    MISTRAL_API_KEY,
    MISTRAL_MODEL,
    DEMO_MODE,
    ENTITY_NORMALIZATION,
    EXCLUDED_TERMS,
    VALID_CATEGORIES,
)
from thales.voice_parser import get_all_segments


def get_mistral_client() -> Optional[Mistral]:
    """
    Get an initialized Mistral API client.

    Returns:
        Initialized Mistral client, or None in demo mode

    Raises:
        ValueError: If MISTRAL_API_KEY is not configured
    """
    if DEMO_MODE:
        return None
    if not MISTRAL_API_KEY:
        raise ValueError(
            "MISTRAL_API_KEY not found in .env file. "
            "Please add MISTRAL_API_KEY=your_api_key to your .env file."
        )
    return Mistral(api_key=MISTRAL_API_KEY)


def normalize_entity(entity: str) -> Optional[str]:
    """
    Normalize an entity to a high-level searchable category.

    Args:
        entity: Raw entity string

    Returns:
        Normalized entity string, or None if the entity should be excluded
    """
    entity_lower = entity.lower().strip()

    # Remove parenthetical descriptions like "(military truck)" or "(operator)"
    entity_clean = re.sub(r'\s*\([^)]*\)\s*', '', entity_lower).strip()

    # Check if this is an excluded term
    for excluded in EXCLUDED_TERMS:
        if excluded in entity_clean:
            return None

    # Check for direct normalization match
    if entity_clean in ENTITY_NORMALIZATION:
        return ENTITY_NORMALIZATION[entity_clean]

    # Check for partial matches in normalization
    for key, normalized in ENTITY_NORMALIZATION.items():
        if key in entity_clean:
            return normalized

    # Check for license plate pattern (letters and numbers, must include digits)
    plate_candidate = entity.strip().upper()
    if re.match(r'^[A-Z0-9]{5,10}$', plate_candidate) and any(ch.isdigit() for ch in plate_candidate):
        return plate_candidate

    # Keep the original if it's a valid high-level category
    if entity_clean in VALID_CATEGORIES:
        return entity_clean

    # For specific vehicle models like "AS 90", "M1 Abrams", keep them
    if re.match(r'^[A-Z]{1,3}[\s-]?\d+', entity.strip().upper()):
        return entity.strip()

    # Default: return the cleaned entity
    return entity_clean if len(entity_clean) > 1 else None


def extract_entities_from_text_local(text: str) -> List[str]:
    """
    Extract entities from text without external APIs (demo mode).

    Args:
        text: Input text to analyze

    Returns:
        List of raw entity candidates found in the text
    """
    if not text or len(text.strip()) == 0:
        return []

    text_lower = text.lower()
    entities = set()

    for term in ENTITY_NORMALIZATION:
        if term in text_lower:
            entities.add(term)

    for category in VALID_CATEGORIES:
        if category in text_lower:
            entities.add(category)

    for match in re.finditer(r"\b[A-Z0-9]{5,10}\b", text.upper()):
        candidate = match.group(0)
        if any(ch.isdigit() for ch in candidate):
            entities.add(candidate)

    for match in re.finditer(r"\b[A-Z]{1,3}[\s-]?\d+\b", text, re.IGNORECASE):
        entities.add(match.group(0).upper())

    return sorted(entities)


def extract_entities_from_text(text: str, client: Optional[Mistral]) -> List[str]:
    """
    Extract military-relevant entities from text using Mistral LLM.

    Args:
        text: Input text to analyze
        client: Initialized Mistral client, or None in demo mode

    Returns:
        List of entity names found in the text
    """
    if not text or len(text.strip()) == 0:
        return []

    if client is None:
        return extract_entities_from_text_local(text)

    text_snippet = text[:2000] if len(text) > 2000 else text

    prompt = f"""Extract all military-relevant entities from this text and categorize them using HIGH-LEVEL, SEARCHABLE terms.

IMPORTANT RULES:
1. Use general categories, NOT specific descriptions
2. Normalize similar items to the same term
3. Focus on what would be searchable in a database
4. DISTINGUISH between military personnel and civilians

CATEGORY MAPPINGS (use these exact terms when applicable):
- Driver, operator, signaler, crew member, technician ‚Üí "military personnel"
- Commander, officer, soldier, gunner, loader ‚Üí "military personnel"
- Any person working with military equipment ‚Üí "military personnel"
- Civilian, bystander, passerby, spectator ‚Üí "civilian"
- Any military truck (semi, transport, logistics, DAF, etc.) ‚Üí "military truck"
- Any tank, armored vehicle ‚Üí "armored vehicle"
- Self-propelled artillery (AS 90, M109, etc.) ‚Üí "artillery vehicle"
- Trailer, flatbed, low loader ‚Üí "trailer"
- Helicopter ‚Üí "helicopter"
- Fixed-wing aircraft ‚Üí "aircraft"
- Drone, UAV ‚Üí "drone"
- Gun, cannon, missile, weapon system ‚Üí "weapon"
- Turret, gun barrel ‚Üí "turret"
- Tracks, wheels, hull (vehicle components) ‚Üí DO NOT include separately, they are part of vehicles
- License plates ‚Üí include as the plate number only (e.g., "AAB960A")
- Clothing items ‚Üí DO NOT include
- Generic descriptions ‚Üí DO NOT include

Text:
{text_snippet}

Return ONLY a JSON object with the entity list. Format: {{"entities": ["military personnel", "military truck", "artillery vehicle"]}}
Do NOT add descriptions in parentheses. Keep entities simple and searchable.
If no entities found, return: {{"entities": []}}
"""

    try:
        response = client.chat.complete(
            model=MISTRAL_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )

        content = response.choices[0].message.content.strip()

        try:
            parsed = json.loads(content)
            if isinstance(parsed, dict):
                if 'entities' in parsed and isinstance(parsed['entities'], list):
                    return [str(e).strip() for e in parsed['entities'] if e]
                for key in ['entity_list', 'result', 'entities_found', 'items']:
                    if key in parsed and isinstance(parsed[key], list):
                        return [str(e).strip() for e in parsed[key] if e]
                for value in parsed.values():
                    if isinstance(value, list):
                        return [str(e).strip() for e in value if e]
            elif isinstance(parsed, list):
                return [str(e).strip() for e in parsed if e]
        except json.JSONDecodeError:
            # Try to extract array from text
            array_match = re.search(r'\[(.*?)\]', content, re.DOTALL)
            if array_match:
                array_content = array_match.group(1)
                entities = []
                for match in re.finditer(r'["\']([^"\']+)["\']|(\w+(?:\s+\w+)*)', array_content):
                    entity = match.group(1) or match.group(2)
                    if entity:
                        entities.append(entity.strip())
                if entities:
                    return entities

        print(f"Warning: Could not parse entity extraction response: {content[:200]}")
        return []

    except Exception as e:
        print(f"Error extracting entities with Mistral: {e}")
        return []


def extract_military_entities(voice_file_path: str) -> Set[str]:
    """
    Extract all military-relevant entities from a voice file.

    Args:
        voice_file_path: Path to the voice file

    Returns:
        Set of unique normalized entity names
    """
    print("Initializing Mistral client...")
    client = get_mistral_client()
    if client is None:
        print("Demo mode enabled: using local entity extraction.")

    print(f"Parsing voice file: {voice_file_path}")
    segments = get_all_segments(voice_file_path)

    all_entities = set()

    if client is None:
        print(f"Processing {len(segments)} segments with local extractor...")
    else:
        print(f"Processing {len(segments)} segments with Mistral LLM...")
    for i, (timestamp, text) in enumerate(segments):
        if i % 10 == 0:
            print(f"  Processing segment {i+1}/{len(segments)}")

        entities = extract_entities_from_text(text, client)

        for entity in entities:
            normalized = normalize_entity(entity)
            if normalized:
                all_entities.add(normalized)

    print(f"Found {len(all_entities)} unique entities")
    return all_entities


def get_entity_list(voice_file_path: str) -> List[str]:
    """
    Get a sorted list of unique military entities from a voice file.

    Args:
        voice_file_path: Path to the voice file

    Returns:
        Sorted list of entity names
    """
    entities = extract_military_entities(voice_file_path)
    return sorted(list(entities))


def extract_entities_with_context(voice_file_path: str) -> Dict[str, List[str]]:
    """
    Extract entities with their surrounding context from a voice file.

    Args:
        voice_file_path: Path to the voice file

    Returns:
        Dictionary mapping entity names to lists of context strings
    """
    print("Initializing Mistral client...")
    client = get_mistral_client()
    if client is None:
        print("Demo mode enabled: using local entity extraction.")

    print(f"Parsing voice file: {voice_file_path}")
    segments = get_all_segments(voice_file_path)

    entity_contexts: Dict[str, List[str]] = {}

    if client is None:
        print(f"Processing {len(segments)} segments with local extractor...")
    else:
        print(f"Processing {len(segments)} segments with Mistral LLM...")
    for i, (timestamp, text) in enumerate(segments):
        if i % 10 == 0:
            print(f"  Processing segment {i+1}/{len(segments)}")

        entities = extract_entities_from_text(text, client)

        for entity_text in entities:
            normalized = normalize_entity(entity_text)
            if not normalized:
                continue

            try:
                entity_lower = entity_text.strip().lower()
                text_lower = text.lower()

                start_pos = 0
                while True:
                    entity_index = text_lower.find(entity_lower, start_pos)
                    if entity_index < 0:
                        break

                    start = max(0, entity_index - 300)
                    end = min(len(text), entity_index + len(entity_text) + 300)
                    context = text[start:end].strip()

                    if normalized not in entity_contexts:
                        entity_contexts[normalized] = []
                    entity_contexts[normalized].append(context)

                    start_pos = entity_index + len(entity_text)

            except Exception as e:
                print(f"Error extracting context for entity '{entity_text}': {e}")
                continue

    print(f"Found {len(entity_contexts)} unique entities with context")
    return entity_contexts

```

## Code: thales/report_generator.py

```python
"""
Report generation for entity detection results.
"""

import json
from typing import Dict, List, Any
from pathlib import Path


def seconds_to_timestamp(seconds: int) -> str:
    """
    Convert seconds to MM:SS timestamp format.

    Args:
        seconds: Number of seconds

    Returns:
        Timestamp string in MM:SS format
    """
    minutes = seconds // 60
    secs = seconds % 60
    return f"{minutes:02d}:{secs:02d}"


def generate_report(
    video_path: str,
    detection_results: Dict[str, List[Dict[str, Any]]],
    output_path: str = None
) -> Dict:
    """
    Generate a JSON report from detection results.

    Args:
        video_path: Path to the video file
        detection_results: Dictionary mapping entity names to detection lists
        output_path: Optional path to save the report JSON file

    Returns:
        Dictionary containing the report data
    """
    video_name = Path(video_path).name

    report = {
        "video": video_name,
        "video_path": str(video_path),
        "entities": {}
    }

    for entity, detections in detection_results.items():
        present_detections = [d for d in detections if d['present']]

        total_frames = len(detections)
        present_frames = len(present_detections)
        presence_percentage = (present_frames / total_frames * 100) if total_frames > 0 else 0

        # Calculate time ranges where entity is present
        time_ranges = []
        if present_detections:
            current_range_start = None
            current_range_end = None

            for det in detections:
                if det['present']:
                    if current_range_start is None:
                        current_range_start = det['second']
                    current_range_end = det['second']
                else:
                    if current_range_start is not None:
                        time_ranges.append({
                            "start": seconds_to_timestamp(current_range_start),
                            "end": seconds_to_timestamp(current_range_end),
                            "start_second": current_range_start,
                            "end_second": current_range_end,
                            "duration_seconds": current_range_end - current_range_start + 1
                        })
                        current_range_start = None
                        current_range_end = None

            if current_range_start is not None:
                time_ranges.append({
                    "start": seconds_to_timestamp(current_range_start),
                    "end": seconds_to_timestamp(current_range_end),
                    "start_second": current_range_start,
                    "end_second": current_range_end,
                    "duration_seconds": current_range_end - current_range_start + 1
                })

        report["entities"][entity] = {
            "statistics": {
                "total_frames_analyzed": total_frames,
                "frames_with_entity": present_frames,
                "presence_percentage": round(presence_percentage, 2)
            },
            "time_ranges": time_ranges,
            "detections": detections
        }

    if output_path:
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"Report saved to: {output_path}")

    return report


def generate_summary_report(reports: List[Dict], output_path: str = None) -> Dict:
    """
    Generate a summary report combining multiple video reports.

    Args:
        reports: List of individual video reports
        output_path: Optional path to save the summary report

    Returns:
        Dictionary containing the summary report
    """
    summary = {
        "total_videos": len(reports),
        "videos": []
    }

    all_entities = set()
    for report in reports:
        all_entities.update(report["entities"].keys())
        summary["videos"].append({
            "video": report["video"],
            "entities_detected": list(report["entities"].keys()),
            "entity_count": len(report["entities"])
        })

    summary["all_entities"] = sorted(list(all_entities))
    summary["unique_entity_count"] = len(all_entities)

    if output_path:
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        print(f"Summary report saved to: {output_path}")

    return summary


```

## Code: thales/utils_io.py

```python
from __future__ import annotations

import codecs
from pathlib import Path
from typing import Optional


def detect_text_encoding_from_bom(data: bytes) -> Optional[str]:
    """
    Detect text encoding from BOM if present.

    Returns an encoding string or None if no BOM is found.
    """
    if data.startswith(codecs.BOM_UTF32_LE):
        return "utf-32le"
    if data.startswith(codecs.BOM_UTF32_BE):
        return "utf-32be"
    if data.startswith(codecs.BOM_UTF16_LE):
        return "utf-16le"
    if data.startswith(codecs.BOM_UTF16_BE):
        return "utf-16be"
    if data.startswith(codecs.BOM_UTF8):
        return "utf-8-sig"
    return None


def read_text_robust(path: str) -> str:
    """
    Read text from a file with robust encoding handling.

    This function never raises UnicodeDecodeError and normalizes line endings.
    """
    data = Path(path).read_bytes()

    encoding = detect_text_encoding_from_bom(data)
    encodings = []
    if encoding:
        encodings.append(encoding)

    for candidate in [
        "utf-8",
        "utf-8-sig",
        "utf-16",
        "utf-16le",
        "utf-16be",
        "cp1252",
        "latin-1",
    ]:
        if candidate not in encodings:
            encodings.append(candidate)

    text = None
    for enc in encodings:
        try:
            text = data.decode(enc)
            break
        except UnicodeDecodeError:
            continue

    if text is None:
        text = data.decode("utf-8", errors="replace")

    text = text.replace("\r\n", "\n").replace("\r", "\n")

    while text.startswith("\ufeff"):
        text = text[1:]

    replacement_count = text.count("\ufffd")
    if text and (replacement_count / max(1, len(text))) > 0.01:
        print(
            "Warning: Transcript encoding may be corrupted for "
            f"{path} ({replacement_count} replacement characters)."
        )

    return text

```

## Code: thales/video_processor.py

```python
"""
Video processing utilities for frame extraction.
"""

import cv2
import numpy as np
from typing import List, Tuple, Optional


def get_video_duration(video_path: str) -> float:
    """
    Get the duration of a video in seconds.

    Args:
        video_path: Path to the video file

    Returns:
        Duration in seconds
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"Could not open video: {video_path}")

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    duration = frame_count / fps if fps > 0 else 0

    cap.release()
    return duration


def extract_frame_at_second(video_path: str, second: int) -> Optional[np.ndarray]:
    """
    Extract a single frame from the video at a specific second.

    Args:
        video_path: Path to the video file
        second: Second number (0-indexed)

    Returns:
        Frame as numpy array (BGR format), or None if extraction fails
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video {video_path}")
        return None

    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps <= 0:
        print(f"Error: Invalid FPS for video {video_path}")
        cap.release()
        return None

    # Calculate frame number for the given second
    frame_number = int(second * fps)

    # Set the video position to the desired frame
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)

    ret, frame = cap.read()
    cap.release()

    if ret:
        return frame
    else:
        print(f"Warning: Could not read frame at second {second}")
        return None


def extract_frames_at_intervals(
    video_path: str,
    interval_seconds: int = 1
) -> List[Tuple[int, np.ndarray]]:
    """
    Extract frames from video at regular intervals.

    Args:
        video_path: Path to the video file
        interval_seconds: Interval between frames in seconds (default: 1)

    Returns:
        List of tuples (second, frame) for each extracted frame
    """
    duration = get_video_duration(video_path)
    total_seconds = int(duration)

    print(f"Video duration: {duration:.2f} seconds")
    print(f"Extracting frames every {interval_seconds} second(s)...")

    frames = []
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        raise ValueError(f"Could not open video: {video_path}")

    fps = cap.get(cv2.CAP_PROP_FPS)

    for second in range(0, total_seconds + 1, interval_seconds):
        frame_number = int(second * fps)
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)

        ret, frame = cap.read()
        if ret:
            frames.append((second, frame))
            if (second + 1) % 10 == 0:
                print(f"  Extracted {second + 1}/{total_seconds + 1} frames...")
        else:
            print(f"Warning: Could not read frame at second {second}")

    cap.release()
    print(f"Extracted {len(frames)} frames total")
    return frames


def seconds_to_timestamp(seconds: int) -> str:
    """
    Convert seconds to MM:SS timestamp format.

    Args:
        seconds: Number of seconds

    Returns:
        Timestamp string in MM:SS format
    """
    minutes = seconds // 60
    secs = seconds % 60
    return f"{minutes:02d}:{secs:02d}"


```

## Code: thales/voice_parser.py

```python
"""
Voice file parser for extracting timestamped segments from transcripts.
"""

import re
from typing import Dict, List, Tuple
from pathlib import Path

from thales.utils_io import read_text_robust


def parse_voice_file(voice_file_path: str) -> Dict[str, str]:
    """
    Parse a voice file to extract timestamped segments.

    Args:
        voice_file_path: Path to the voice file

    Returns:
        Dictionary mapping timestamps (MM:SS format) to text descriptions
    """
    content = read_text_robust(voice_file_path)

    # Pattern to match timestamps like "Speaker 1  (00:01)" or "(01:23)"
    timestamp_pattern = r'\((\d{2}):(\d{2})\)'

    segments = {}
    lines = content.split('\n')
    current_timestamp = None
    current_text = []

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # Check if line contains a timestamp
        match = re.search(timestamp_pattern, line)
        if match:
            # Save previous segment if exists
            if current_timestamp is not None and current_text:
                segments[current_timestamp] = ' '.join(current_text)

            # Extract timestamp
            minutes, seconds = match.groups()
            current_timestamp = f"{minutes}:{seconds}"
            current_text = []

            # Extract text after timestamp on the same line
            text_after_timestamp = line[match.end():].strip()
            if text_after_timestamp:
                current_text.append(text_after_timestamp)
        else:
            # Continue accumulating text for current segment
            if current_timestamp is not None:
                current_text.append(line)

    # Don't forget the last segment
    if current_timestamp is not None and current_text:
        segments[current_timestamp] = ' '.join(current_text)

    return segments


def get_all_segments(voice_file_path: str) -> List[Tuple[str, str]]:
    """
    Get all timestamped segments as a list of (timestamp, text) tuples.

    Args:
        voice_file_path: Path to the voice file

    Returns:
        List of (timestamp, text) tuples, sorted by timestamp
    """
    segments = parse_voice_file(voice_file_path)

    # Convert to list and sort by timestamp
    result = []
    for timestamp, text in segments.items():
        minutes, seconds = map(int, timestamp.split(':'))
        total_seconds = minutes * 60 + seconds
        result.append((timestamp, text, total_seconds))

    # Sort by total seconds
    result.sort(key=lambda x: x[2])

    return [(ts, text) for ts, text, _ in result]

```

## Code: ui/app.py

```python
import json
import sys
import time
from pathlib import Path

import pandas as pd
import streamlit as st

from utils import ALLOWED_VIDEO_EXTS, find_pairs, run_pipeline

ROOT_DIR = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT_DIR / "data"
WORK_DIR = ROOT_DIR / "ui" / "work"


def load_json(path: Path):
    try:
        with open(path, "r", encoding="utf-8") as handle:
            return json.load(handle)
    except Exception:
        return None


def format_pair_label(pair: dict) -> str:
    voice_name = Path(pair["voice_path"]).name
    video_name = Path(pair["video_path"]).name
    return f"{pair['pair_id']}: {voice_name} + {video_name}"


def ensure_work_dir():
    WORK_DIR.mkdir(parents=True, exist_ok=True)


def validate_transcript(text_bytes: bytes) -> bool:
    try:
        return bool(text_bytes.decode("utf-8", errors="ignore").strip())
    except Exception:
        return False


st.set_page_config(page_title="Thales Video Indexing UI", layout="wide")
ensure_work_dir()

st.title("Thales Video Indexing")
st.write("Run the entity detection pipeline without using the command line.")

st.sidebar.header("Input")
input_mode = st.sidebar.radio("Mode", ["Upload files", "Use existing data/"])

selected_pair_id = None
data_dir = DATA_DIR
upload_run_dir = None

if input_mode == "Upload files":
    voice_upload = st.sidebar.file_uploader(
        "Transcript file (voice_*.txt)", type=["txt"]
    )
    video_upload = st.sidebar.file_uploader(
        "Video file", type=[ext.strip(".") for ext in ALLOWED_VIDEO_EXTS]
    )
    pair_number = st.sidebar.number_input(
        "Pair number", min_value=1, value=1, step=1
    )
else:
    pairs = find_pairs(DATA_DIR)
    if not pairs:
        st.sidebar.warning("No matching pairs found in data/.")
    else:
        labels = {format_pair_label(pair): pair for pair in pairs}
        selected_label = st.sidebar.selectbox("Select pair", list(labels.keys()))
        selected_pair_id = labels[selected_label]["pair_id"]

st.sidebar.header("Settings")
frame_interval = st.sidebar.number_input(
    "Frame interval (seconds)", min_value=1, value=30, step=1
)
output_dir_input = st.sidebar.text_input("Output directory", value="reports_ui")
demo_mode = st.sidebar.checkbox("Demo mode", value=False)
st.sidebar.caption(
    "Demo mode skips external API calls. Turn it off for real model results."
)

run_button = st.button("Run pipeline", type="primary")
log_placeholder = st.empty()

if "logs" not in st.session_state:
    st.session_state["logs"] = ""
if "produced_files" not in st.session_state:
    st.session_state["produced_files"] = {}
if "returncode" not in st.session_state:
    st.session_state["returncode"] = None
if "show_logs_expanded" not in st.session_state:
    st.session_state["show_logs_expanded"] = False

if run_button:
    st.session_state["logs"] = ""
    st.session_state["produced_files"] = {}
    st.session_state["returncode"] = None

    errors = []

    if input_mode == "Upload files":
        if voice_upload is None or video_upload is None:
            errors.append("Upload both the transcript and video files.")
        else:
            voice_bytes = voice_upload.getvalue()
            if not validate_transcript(voice_bytes):
                errors.append("Transcript file is empty or invalid.")

            video_ext = Path(video_upload.name).suffix.lower()
            if video_ext not in ALLOWED_VIDEO_EXTS:
                errors.append(
                    f"Video extension {video_ext} is not supported. "
                    f"Allowed: {', '.join(ALLOWED_VIDEO_EXTS)}"
                )

            if not errors:
                timestamp = int(time.time())
                upload_run_dir = WORK_DIR / f"run_{timestamp}_pair_{int(pair_number)}"
                upload_run_dir.mkdir(parents=True, exist_ok=True)

                voice_path = upload_run_dir / f"voice_{int(pair_number)}.txt"
                video_path = upload_run_dir / f"video_{int(pair_number)}{video_ext}"

                voice_path.write_bytes(voice_bytes)
                video_path.write_bytes(video_upload.getvalue())

                data_dir = upload_run_dir
                selected_pair_id = None
    else:
        pairs = find_pairs(DATA_DIR)
        if not pairs:
            errors.append("No valid pairs found in data/.")
        elif selected_pair_id is None:
            errors.append("Select a voice/video pair.")
        else:
            selected = next(
                (p for p in pairs if p["pair_id"] == str(selected_pair_id)), None
            )
            if selected:
                transcript_path = Path(selected["voice_path"])
                if not validate_transcript(transcript_path.read_bytes()):
                    errors.append("Selected transcript file is empty.")
            else:
                errors.append("Selected pair could not be resolved.")

    if errors:
        for err in errors:
            st.error(err)
    else:
        demo_value = "1" if demo_mode else "0"
        env_overrides = {
            "DEMO_MODE": demo_value,
            "THALES_DEMO_MODE": demo_value,
        }

        out_dir = Path(output_dir_input)
        if not out_dir.is_absolute():
            out_dir = ROOT_DIR / out_dir

        def on_log(_line, logs_text):
            st.session_state["logs"] = logs_text
            log_placeholder.code(logs_text)

        returncode, logs_text, produced_files = run_pipeline(
            sys.executable,
            data_dir,
            int(frame_interval),
            out_dir,
            env_overrides,
            selected_pair_id=selected_pair_id,
            log_callback=on_log,
        )

        st.session_state["logs"] = logs_text
        st.session_state["produced_files"] = produced_files
        st.session_state["returncode"] = returncode

        outputs_found = bool(
            produced_files.get("summary_report") or produced_files.get("video_report")
        )

        if returncode != 0:
            st.error("Pipeline failed. Check the logs below for details.")
            st.session_state["show_logs_expanded"] = True
        elif outputs_found:
            st.success("Pipeline completed successfully.")
            st.session_state["show_logs_expanded"] = False
        else:
            st.warning(
                "Pipeline finished but no output files were found. "
                "Check the logs below for details."
            )
            st.session_state["show_logs_expanded"] = True

if st.session_state.get("logs"):
    with st.expander("Logs", expanded=st.session_state.get("show_logs_expanded", False)):
        st.code(st.session_state["logs"])

produced_files = st.session_state.get("produced_files", {})
summary_path = produced_files.get("summary_report")
video_report_path = produced_files.get("video_report")

if summary_path or video_report_path:
    st.header("Results")

summary_data = load_json(Path(summary_path)) if summary_path else None
video_report_data = load_json(Path(video_report_path)) if video_report_path else None

if summary_data or video_report_data:
    entity_count = None
    total_videos = None
    total_confirmed_frames = None

    if summary_data:
        entity_count = summary_data.get("unique_entity_count")
        total_videos = summary_data.get("total_videos")

    if video_report_data:
        entities = video_report_data.get("entities", {})
        if entity_count is None:
            entity_count = len(entities)
        total_confirmed_frames = sum(
            data.get("statistics", {}).get("frames_with_entity", 0)
            for data in entities.values()
        )

    cols = st.columns(3)
    cols[0].metric("Entities", entity_count if entity_count is not None else "N/A")
    cols[1].metric("Videos processed", total_videos if total_videos is not None else "N/A")
    cols[2].metric(
        "Confirmed frames",
        total_confirmed_frames if total_confirmed_frames is not None else "N/A",
    )

    if video_report_data:
        entities = video_report_data.get("entities", {})
        rows = []
        for name, data in entities.items():
            stats = data.get("statistics", {})
            detections = data.get("detections", [])
            present = [d for d in detections if d.get("present")]
            present_sorted = sorted(present, key=lambda d: d.get("second", 0))

            first_seen = None
            last_seen = None
            if present_sorted:
                first_seen = present_sorted[0].get("timestamp")
                last_seen = present_sorted[-1].get("timestamp")
            elif data.get("time_ranges"):
                first_seen = data["time_ranges"][0].get("start")
                last_seen = data["time_ranges"][-1].get("end")

            rows.append(
                {
                    "entity": name,
                    "first_seen": first_seen or "N/A",
                    "last_seen": last_seen or "N/A",
                    "frames_confirmed": stats.get("frames_with_entity", 0),
                    "presence_percent": stats.get("presence_percentage", "N/A"),
                }
            )

        st.subheader("Entities")
        st.dataframe(pd.DataFrame(rows), use_container_width=True)

        entity_names = list(entities.keys())
        if entity_names:
            selected_entity = st.selectbox("Entity timeline", entity_names)
            entity_data = entities.get(selected_entity, {})
            detections = entity_data.get("detections", [])

            st.subheader(f"Timeline: {selected_entity}")
            if detections:
                timeline_rows = [
                    {"second": d.get("second", 0), "present": int(d.get("present", False))}
                    for d in detections
                ]
                timeline_df = pd.DataFrame(timeline_rows).sort_values("second")
                st.line_chart(timeline_df.set_index("second"))

                present_times = [
                    d.get("timestamp") for d in detections if d.get("present")
                ]
                if present_times:
                    st.write("Present at:", ", ".join(present_times))
                else:
                    st.write("No confirmed frames for this entity.")
            elif entity_data.get("time_ranges"):
                st.write("Time ranges:")
                st.table(entity_data.get("time_ranges"))
            else:
                st.info("No timeline data available for this entity.")

if summary_path and Path(summary_path).exists():
    with open(summary_path, "rb") as handle:
        st.download_button(
            "Download summary_report.json",
            handle,
            file_name=Path(summary_path).name,
        )

if video_report_path and Path(video_report_path).exists():
    with open(video_report_path, "rb") as handle:
        st.download_button(
            "Download video report JSON",
            handle,
            file_name=Path(video_report_path).name,
        )

```

## Code: ui/utils.py

```python
from __future__ import annotations

import os
import re
import shutil
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Optional

ALLOWED_VIDEO_EXTS = [".mp4", ".mkv", ".avi", ".mov"]


def find_pairs(data_dir: Path) -> List[Dict[str, str]]:
    """
    Find matching voice/video pairs in a directory.

    Returns a list of dicts with pair_id, voice_path, and video_path.
    """
    data_dir = Path(data_dir)
    if not data_dir.exists():
        return []

    pairs: List[Dict[str, str]] = []
    voice_files = sorted(data_dir.glob("voice_*.txt"))

    for voice_file in voice_files:
        match = re.match(r"voice_(\d+)\.txt$", voice_file.name)
        if not match:
            continue
        pair_id = match.group(1)

        video_file = None
        for ext in ALLOWED_VIDEO_EXTS:
            candidate = data_dir / f"video_{pair_id}{ext}"
            if candidate.exists():
                video_file = candidate
                break

        if video_file:
            pairs.append(
                {
                    "pair_id": pair_id,
                    "voice_path": str(voice_file),
                    "video_path": str(video_file),
                }
            )

    return pairs


def run_pipeline(
    python_exe: str,
    data_dir: Path,
    interval: int,
    out_dir: Path,
    env_overrides: Optional[Dict[str, str]],
    selected_pair_id: Optional[str] = None,
    log_callback=None,
):
    """
    Run the CLI pipeline and return (returncode, logs_text, produced_files).
    """
    root_dir = Path(__file__).resolve().parents[1]
    data_dir = Path(data_dir)
    out_dir = Path(out_dir)

    if not data_dir.is_absolute():
        data_dir = root_dir / data_dir
    if not out_dir.is_absolute():
        out_dir = root_dir / out_dir

    out_dir.mkdir(parents=True, exist_ok=True)

    run_dir = data_dir
    selected_video_path: Optional[Path] = None

    if selected_pair_id is not None:
        pairs = find_pairs(data_dir)
        pair_id = str(selected_pair_id)
        selected = next((p for p in pairs if p["pair_id"] == pair_id), None)
        if not selected:
            return 1, f"Pair {pair_id} not found in {data_dir}", {}

        work_dir = root_dir / "ui" / "work"
        work_dir.mkdir(parents=True, exist_ok=True)

        run_dir = work_dir / f"run_{int(time.time())}_pair_{pair_id}"
        run_dir.mkdir(parents=True, exist_ok=True)

        voice_src = Path(selected["voice_path"])
        video_src = Path(selected["video_path"])
        selected_video_path = video_src

        shutil.copy2(voice_src, run_dir / f"voice_{pair_id}.txt")
        shutil.copy2(video_src, run_dir / f"video_{pair_id}{video_src.suffix}")
    else:
        pairs = find_pairs(data_dir)
        if len(pairs) == 1:
            selected_video_path = Path(pairs[0]["video_path"])

    cmd = [
        python_exe,
        "-u",
        "-m",
        "thales",
        "-d",
        str(run_dir),
        "-i",
        str(interval),
        "-o",
        str(out_dir),
    ]

    env = os.environ.copy()
    env["PYTHONUNBUFFERED"] = "1"
    if env_overrides:
        env.update(env_overrides)

    proc = subprocess.Popen(
        cmd,
        cwd=root_dir,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
    )

    logs: List[str] = []
    if proc.stdout:
        for line in iter(proc.stdout.readline, ""):
            logs.append(line)
            if log_callback:
                log_callback(line, "".join(logs))
        proc.stdout.close()

    returncode = proc.wait()
    logs_text = "".join(logs)

    produced_files: Dict[str, str] = {}
    summary_path = out_dir / "summary_report.json"
    if summary_path.exists():
        produced_files["summary_report"] = str(summary_path)

    video_report = None
    if selected_video_path:
        candidate = out_dir / f"{selected_video_path.stem}_report.json"
        if candidate.exists():
            video_report = candidate
    else:
        candidates = [
            p for p in out_dir.glob("*_report.json") if p.name != "summary_report.json"
        ]
        if candidates:
            video_report = max(candidates, key=lambda p: p.stat().st_mtime)

    if video_report and video_report.exists():
        produced_files["video_report"] = str(video_report)

    return returncode, logs_text, produced_files

```

## Sample Outputs (Reports)

### /Users/syedtashfin/Documents/GitHub/thales-optronic-video-indexing-main/reports_ui/summary_report.json

```json
{
  "total_videos": 1,
  "videos": [
    {
      "video": "video_1.mp4",
      "entities_detected": [
        "AAB960A",
        "artillery vehicle",
        "civilian",
        "military personnel",
        "military truck",
        "trailer",
        "turret",
        "weapon"
      ],
      "entity_count": 8
    }
  ],
  "all_entities": [
    "AAB960A",
    "artillery vehicle",
    "civilian",
    "military personnel",
    "military truck",
    "trailer",
    "turret",
    "weapon"
  ],
  "unique_entity_count": 8
}
```

### /Users/syedtashfin/Documents/GitHub/thales-optronic-video-indexing-main/reports/video_1_report.json

```json
{
  "video": "video_1.mkv",
  "video_path": "video_1.mkv",
  "entities": {
    "AAB960A": {
      "statistics": {
        "total_frames_analyzed": 22,
        "frames_with_entity": 8,
        "presence_percentage": 36.36
      },
      "time_ranges": [
        {
          "start": "00:00",
          "end": "00:05",
          "start_second": 0,
          "end_second": 5,
          "duration_seconds": 6
        },
        {
          "start": "00:15",
          "end": "00:20",
          "start_second": 15,
          "end_second": 20,
          "duration_seconds": 6
        },
        {
          "start": "00:30",
          "end": "00:40",
          "start_second": 30,
          "end_second": 40,
          "duration_seconds": 11
        },
        {
          "start": "01:20",
          "end": "01:20",
          "start_second": 80,
          "end_second": 80,
          "duration_seconds": 1
        }
      ],
      "detections": [
        {
          "timestamp": "00:00",
          "second": 0,
          "present": true
        },
        {
          "timestamp": "00:05",
          "second": 5,
          "present": true
        },
        {
          "timestamp": "00:10",
          "second": 10,
          "present": false
        },
        {
          "timestamp": "00:15",
          "second": 15,
          "present": true
        },
        {
          "timestamp": "00:20",
          "second": 20,
          "present": true
        },
        {
          "timestamp": "00:25",
          "second": 25,
          "present": false
        },
        {
          "timestamp": "00:30",
          "second": 30,
          "present": true
        },
        {
          "timestamp": "00:35",
          "second": 35,
          "present": true
        },
        {
          "timestamp": "00:40",
          "second": 40,
          "present": true
        },
        {
          "timestamp": "00:45",
          "second": 45,
          "present": false
        },
        {
          "timestamp": "00:50",
          "second": 50,
          "present": false
        },
        {
          "timestamp": "00:55",
          "second": 55,
          "present": false
        },
        {
          "timestamp": "01:00",
          "second": 60,
          "present": false
        },
        {
          "timestamp": "01:05",
          "second": 65,
          "present": false
        },
        {
          "timestamp": "01:10",
          "second": 70,
          "present": false
        },
        {
          "timestamp": "01:15",
          "second": 75,
          "present": false
        },
        {
          "timestamp": "01:20",
          "second": 80,
          "present": true
        },
        {
          "timestamp": "01:25",
          "second": 85,
          "present": false
        },
        {
          "timestamp": "01:30",
          "second": 90,
          "present": false
        },
        {
          "timestamp": "01:35",
          "second": 95,
          "present": false
        },
        {
          "timestamp": "01:40",
          "second": 100,
          "present": false
        },
        {
          "timestamp": "01:45",
          "second": 105,
          "present": false
... [truncated after 150 lines]
```

## Known Issues

- `MISTRAL_API_KEY` is not set in the current environment.
- First real run may download large Hugging Face models and requires internet.
- Transcript reader warns if replacement characters exceed 1% (encoding issues).

## TODO / Improvement Ideas
